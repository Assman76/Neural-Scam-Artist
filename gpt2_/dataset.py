from sklearn.model_selection import train_test_split
import torch
from utils.text_processing import read_csv_file


class GPT2_dataset(torch.utils.data.Dataset):
    """Dataset class for GPT-2.

    Attributes:
      training: (bool) True if the dataset is used for training, 
        False if used for evaluation.
      max_len: (int) Sequences longer than max_len will be truncated
        to max_len.
      len__: (int) Number of samples stored.
      encodings: (dict) Contains lists of token indices.

    Standard Huggingface datasets store both token indices and masks,
    both padded to maximum length. The author disagrres with this 
    approach as storing masks is meaningless. Instead, the dataset
    stores only truncated but unpadded indices. Masks and padding
    are generated by a data colator which is defined below.
    """

    def __init__(self, corpus, tokenizer, max_len=512, training=True):
        """Initializes a GPT2_dataset.

        Args:
          corpus: (list) List where each element is sample in the form of
            a string.
          tokenizer: (transformers.GPT2Tokenizer) Tokenizer that splits
            strings into tokens.     
          max_len: (int) Sequences longer than max_len will be truncated
            to max_len.
          training: (bool) True if the dataset is used for training, 
            False if used for evaluation.
        """
        super(GPT2_dataset, self).__init__()

        self.training = training
        self.max_len = max_len
        self.len__ = len(corpus)
        name = 'training' if self.training else 'evaluation'
        print(f'Tokenizing {name} dataset...')
        self.encodings = tokenizer(
            corpus, truncation=True, padding=False,
            max_length=self.max_len,  # return_tensors='pt',
            return_attention_mask=False)

    def train(self):
        self.training = True

    def eval(self):
        self.training = False

    def __getitem__(self, idx):
        # Returns a single unpadded list of token indices
        item = {'input_ids': self.encodings['input_ids'][idx]}
        return item

    def __len__(self):
        return self.len__


def get_datasets(
        source_file, tokenizer, n_samples=-1, max_len=512,
        val_size=0.1, shuffle=True):
    """Creates the dataset and splits it into train and val subsets.

    Args:
      source_file: (str) Path to a csv file where the dataset is stored.
      tokenizer: (transformers.GPT2Tokenizer) Tokenizer that splits
        strings into tokens. 
      n_samples: (int) If given, only the first n_samples rows will 
        be read.    
      max_len: (int) Sequences longer than max_len will be truncated
        to max_len.
      val_size: (float) Proportion of the dataset to include in the 
        val split. If val_size == 0, validation set is not created and
        all samples go to the train set.
      shuffle: (bool) Whether or not to shuffle the data before splitting.
        If shuffle == False, the validation set will be the suffix of the 
        dataset.

    Returns:
      train_dataset: (torch.utils.data.Dataset) Training dataset.
      val_dataset (Optional): (torch.utils.data.Dataset) Validation dataset.
    """

    corpus = read_csv_file(source_file, n_samples)
    if val_size == 0:
        train_dataset = GPT2_dataset(corpus, tokenizer, max_len, training=True)
        return train_dataset

    train_corpus, val_corpus = train_test_split(
        corpus, test_size=val_size, shuffle=shuffle)
    train_dataset = GPT2_dataset(
        train_corpus, tokenizer, max_len, training=True)
    val_dataset = GPT2_dataset(val_corpus, tokenizer, max_len, training=False)

    return train_dataset, val_dataset
